{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09708aa6",
   "metadata": {},
   "source": [
    "# 04. PyTorch Custom Datasets\n",
    "\n",
    "In the last notebook, we looked at how to build computer vision models on an in-built dataset in PyTorch (FashionMNIST).\n",
    "\n",
    "The steps we took are similar across many different problems in machine learning.\n",
    "\n",
    "Find a dataset, turn the dataset into numbers, build a model (or find an existing model) to find patterns in those numbers that can be used for prediction.\n",
    "\n",
    "PyTorch has many built-in datasets used for a wide number of machine learning benchmarks, however, you'll often want to use your own **custom dataset**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b7d9e1",
   "metadata": {},
   "source": [
    "## What is a custom dataset?\n",
    "\n",
    "A **custom dataset** is a collection of data relating to a specific problem you're working on.\n",
    "\n",
    "In essence, a **custom dataset** can be comprised of almost anything.\n",
    "\n",
    "For example, if we were building a food image classification app like [Nutrify](https://nutrify.app), our custom dataset might be images of food.\n",
    "\n",
    "Or if we were trying to build a model to classify whether or not a text-based review on a website was positive or negative, our custom dataset might be examples of existing customer reviews and their ratings.\n",
    "\n",
    "Or if we were trying to build a sound classification app, our custom dataset might be sound samples alongside their sample labels.\n",
    "\n",
    "Or if we were trying to build a recommendation system for customers purchasing things on our website, our custom dataset might be examples of products other people have bought.\n",
    "\n",
    "*PyTorch includes many existing functions to load in various custom datasets in the [`TorchVision`](https://pytorch.org/vision/stable/index.html), [`TorchText`](https://pytorch.org/text/stable/index.html), [`TorchAudio`](https://pytorch.org/audio/stable/index.html) and [`TorchRec`](https://pytorch.org/torchrec/) domain libraries.*\n",
    "\n",
    "But sometimes these existing functions may not be enough.\n",
    "\n",
    "In that case, we can always subclass `torch.utils.data.Dataset` and customize it to our liking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c50dc4c",
   "metadata": {},
   "source": [
    "## What we're going to cover\n",
    "\n",
    "\n",
    "| **Topic** | **Contents** |\n",
    "| ----- | ----- |\n",
    "| **0. Importing PyTorch and setting up device-agnostic code** | Let's get PyTorch loaded and then follow best practice to setup our code to be device-agnostic.  |\n",
    "| **1. Get data** | We're going to be using our own **custom dataset** of pizza, steak and sushi images. |\n",
    "| **2. Become one with the data (data preparation)** | At the beginning of any new machine learning problem, it's paramount to understand the data you're working with. Here we'll take some steps to figure out what data we have. |\n",
    "| **3. Transforming data** |Often, the data you get won't be 100% ready to use with a machine learning model, here we'll look at some steps we can take to *transform* our images so they're ready to be used with a model. | \n",
    "| **4. Loading data with `ImageFolder` (option 1)** | PyTorch has many in-built data loading functions for common types of data. `ImageFolder` is helpful if our images are in standard image classification format. |\n",
    "| **5. Loading image data with a custom `Dataset`** | What if PyTorch didn't have an in-built function to load data with? This is where we can build our own custom subclass of `torch.utils.data.Dataset`. |\n",
    "| **6. Other forms of transforms (data augmentation)** | Data augmentation is a common technique for expanding the diversity of your training data. Here we'll explore some of `torchvision`'s in-built data augmentation functions. |\n",
    "| **7. Model 0: TinyVGG without data augmentation** | By this stage, we'll have our data ready, let's build a model capable of fitting it. We'll also create some training and testing functions for training and evaluating our model. |\n",
    "| **8. Exploring loss curves** | Loss curves are a great way to see how your model is training/improving over time. They're also a good way to see if your model is **underfitting** or **overfitting**. |\n",
    "| **9. Model 1: TinyVGG with data augmentation** | By now, we've tried a model *without*, how about we try one *with* data augmentation? |\n",
    "| **10. Compare model results** | Let's compare our different models' loss curves and see which performed better and discuss some options for improving performance. |\n",
    "| **11. Making a prediction on a custom image** | Our model is trained to on a dataset of pizza, steak and sushi images. In this section we'll cover how to use our trained model to predict on an image *outside* of our existing dataset. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0613861",
   "metadata": {},
   "source": [
    "## 0. Importing PyTorch and setting up device-agnostic code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c796a29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0ec3ea",
   "metadata": {},
   "source": [
    "## 1. Get data\n",
    "\n",
    "First things first we need some data.\n",
    "\n",
    "Machine learning is an iterative process, start small, get something working and increase when necessary.\n",
    "\n",
    "The data we're going to be using is a subset of the [Food101 dataset](https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/).\n",
    "\n",
    "Food101 is popular computer vision benchmark as it contains 1000 images of 101 different kinds of foods, totaling 101,000 images (75,750 train and 25,250 test).\n",
    "\n",
    "\n",
    "Instead of 101 food classes though, we're going to start with 3: pizza, steak and sushi.\n",
    "\n",
    "And instead of 1,000 images per class, we're going to start with a random 10% (start small, increase when necessary).\n",
    "\n",
    "If you'd like to see where the data came from you see the following resources:\n",
    "* Original [Food101 dataset and paper website](https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/).\n",
    "* [`torchvision.datasets.Food101`](https://pytorch.org/vision/main/generated/torchvision.datasets.Food101.html) - the version of the data I downloaded for this notebook.\n",
    "* [`extras/04_custom_data_creation.ipynb`](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/04_custom_data_creation.ipynb) - a notebook I used to format the Food101 dataset to use for this notebook.\n",
    "* [`data/pizza_steak_sushi.zip`](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/data/pizza_steak_sushi.zip) - the zip archive of pizza, steak and sushi images from Food101, created with the notebook linked above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c03c6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "data_path = Path(\"data/\")\n",
    "image_path = data_path / \"pizza_steak_sushi\"\n",
    "\n",
    "if image_path.is_dir():\n",
    "    print(f\"{image_path} directory exists.\")\n",
    "else:\n",
    "    print(f\"Did not find {image_path} directory, creating one...\")\n",
    "    image_path.mkdir(parents= True, exist_ok= True)\n",
    "\n",
    "    with open(data_path / \"pizza_steak_sushi.zip\", 'wb') as f:\n",
    "        request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n",
    "        print(\"Downloading pizza, steak, sushi data...\")\n",
    "        f.write(request.content)\n",
    "    f.close\n",
    "\n",
    "    with zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", 'r') as zip_ref:\n",
    "        print(\"Unzipping pizza, steak, sushi data...\")\n",
    "        zip_ref.extractall(image_path)\n",
    "    zip_ref.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0505df0",
   "metadata": {},
   "source": [
    "## 2. Become one with the data (data preparation)\n",
    "\n",
    "```\n",
    "pizza_steak_sushi/ <- overall dataset folder\n",
    "    train/ <- training images\n",
    "        pizza/ <- class name as folder name\n",
    "            image01.jpeg\n",
    "            image02.jpeg\n",
    "            ...\n",
    "        steak/\n",
    "            image24.jpeg\n",
    "            image25.jpeg\n",
    "            ...\n",
    "        sushi/\n",
    "            image37.jpeg\n",
    "            ...\n",
    "    test/ <- testing images\n",
    "        pizza/\n",
    "            image101.jpeg\n",
    "            image102.jpeg\n",
    "            ...\n",
    "        steak/\n",
    "            image154.jpeg\n",
    "            image155.jpeg\n",
    "            ...\n",
    "        sushi/\n",
    "            image167.jpeg\n",
    "            ...\n",
    "```\n",
    "\n",
    "The goal will be to **take this data storage structure and turn it into a dataset usable with PyTorch**.\n",
    "\n",
    "> **Note:** The structure of the data you work with will vary depending on the problem you're working on. But the premise still remains: become one with the data, then find a way to best turn it into a dataset compatible with PyTorch.\n",
    "\n",
    "We can inspect what's in our data directory by writing a small helper function to walk through each of the subdirectories and count the files present.\n",
    "\n",
    "To do so, we'll use Python's in-built [`os.walk()`](https://docs.python.org/3/library/os.html#os.walk). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90170257",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def walk_through_dir(dir_path):\n",
    "    \"\"\"\n",
    "    Walks through dir_path returning its contents.\n",
    "    Args:\n",
    "        dir_path (str or pathlib.Path): target directory\n",
    "  \n",
    "    Returns:\n",
    "        A print out of:\n",
    "            number of subdiretories in dir_path\n",
    "            number of images (files) in each subdirectory\n",
    "            name of each subdirectory\n",
    "    \"\"\"\n",
    "    for dirpath, dirnames, filenames in os.walk(dir_path):\n",
    "        print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4764d01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "walk_through_dir(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306faa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup train and testing paths\n",
    "train_dir = image_path / \"train\"\n",
    "test_dir = image_path / \"test\"\n",
    "\n",
    "train_dir, test_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e43722f",
   "metadata": {},
   "source": [
    "### 2.1 Visualize an image\n",
    "\n",
    "Okay, we've seen how our directory structure is formatted.\n",
    "\n",
    "Now in the spirit of the data explorer, it's time to visualize\n",
    "\n",
    "Let's write some code to:\n",
    "1. Get all of the image paths using [`pathlib.Path.glob()`](https://docs.python.org/3/library/pathlib.html#pathlib.Path.glob) to find all of the files ending in `.jpg`. \n",
    "2. Pick a random image path using Python's [`random.choice()`](https://docs.python.org/3/library/random.html#random.choice).\n",
    "3. Get the image class name using [`pathlib.Path.parent.stem`](https://docs.python.org/3/library/pathlib.html#pathlib.PurePath.parent).\n",
    "4. And since we're working with images, we'll open the random image path using [`PIL.Image.open()`](https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.open) (PIL stands for Python Image Library).\n",
    "5. We'll then show the image and print some metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6383bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "image_path_list = list(image_path.glob(\"*/*/*.jpg\"))\n",
    "\n",
    "random_image_path = random.choice(image_path_list)\n",
    "\n",
    "# 3. Get image class from path name (the image class is the name of the directory where the image is stored)\n",
    "image_class = random_image_path.parent.stem\n",
    "\n",
    "img = Image.open(random_image_path)\n",
    "\n",
    "print(f\"Random image path: {random_image_path}\")\n",
    "print(f\"Image class: {image_class}\")\n",
    "print(f\"Image height: {img.height}\") \n",
    "print(f\"Image width: {img.width}\")\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63606f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Turn the image into an array\n",
    "img_as_array = np.asarray(img)\n",
    "\n",
    "# Plot the image with matplotlib\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(img_as_array)\n",
    "plt.title(f\"Image class: {image_class} | Image shape: {img_as_array.shape} -> [height, width, color_channels]\")\n",
    "plt.axis(False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a83fe8c",
   "metadata": {},
   "source": [
    "## 3. Transforming data \n",
    "\n",
    "Before we can use our image data with PyTorch we need to:\n",
    "\n",
    "1. Turn it into tensors (numerical representations of our images).\n",
    "2. Turn it into a `torch.utils.data.Dataset` and subsequently a `torch.utils.data.DataLoader`, we'll call these `Dataset` and `DataLoader` for short.\n",
    "\n",
    "There are several different kinds of pre-built datasets and dataset loaders for PyTorch, depending on the problem you're working on. \n",
    "\n",
    "| **Problem space** | **Pre-built Datasets and Functions** |\n",
    "| ----- | ----- |\n",
    "| **Vision** | [`torchvision.datasets`](https://pytorch.org/vision/stable/datasets.html) |\n",
    "| **Audio** | [`torchaudio.datasets`](https://pytorch.org/audio/stable/datasets.html) |\n",
    "| **Text** | [`torchtext.datasets`](https://pytorch.org/text/stable/datasets.html) |\n",
    "| **Recommendation system** | [`torchrec.datasets`](https://pytorch.org/torchrec/torchrec.datasets.html) |\n",
    "\n",
    "Since we're working with a vision problem, we'll be looking at `torchvision.datasets` for our data loading functions as well as [`torchvision.transforms`](https://pytorch.org/vision/stable/transforms.html) for preparing our data.\n",
    "\n",
    "Let's import some base libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00cfae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a27db14",
   "metadata": {},
   "source": [
    "### 3.1 Transforming data with `torchvision.transforms`\n",
    "\n",
    "We've got folders of images but before we can use them with PyTorch, we need to convert them into tensors.\n",
    "\n",
    "One of the ways we can do this is by using the `torchvision.transforms` module.\n",
    "\n",
    "`torchvision.transforms` contains many pre-built methods for formatting images, turning them into tensors and even manipulating them for **data augmentation** (the practice of altering data to make it harder for a model to learn, we'll see this later on) purposes . \n",
    "\n",
    "To get experience with `torchvision.transforms`, let's write a series of transform steps that:\n",
    "1. Resize the images using [`transforms.Resize()`](https://pytorch.org/vision/stable/generated/torchvision.transforms.Resize.html#torchvision.transforms.Resize) (from about 512x512 to 64x64, the same shape as the images on the [CNN Explainer website](https://poloclub.github.io/cnn-explainer/)).\n",
    "2. Flip our images randomly on the horizontal using [`transforms.RandomHorizontalFlip()`](https://pytorch.org/vision/stable/generated/torchvision.transforms.RandomHorizontalFlip.html#torchvision.transforms.RandomHorizontalFlip) (this could be considered a form of data augmentation because it will artificially change our image data).\n",
    "3. Turn our images from a PIL image to a PyTorch tensor using [`transforms.ToTensor()`](https://pytorch.org/vision/stable/generated/torchvision.transforms.ToTensor.html#torchvision.transforms.ToTensor).\n",
    "\n",
    "We can compile all of these steps using [`torchvision.transforms.Compose()`](https://pytorch.org/vision/stable/generated/torchvision.transforms.Compose.html#torchvision.transforms.Compose)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e58ffb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write transform for image\n",
    "data_transform = transforms.Compose([\n",
    "    # Resize the images to 64x64\n",
    "    transforms.Resize(size = (64, 64)),\n",
    "    # Flip the images randomly on the horizontal\n",
    "    transforms.RandomHorizontalFlip(p = 0.5), # p = probability of flip, 0.5 = 50% chance\n",
    "    # Turn the image into a torch.Tensor\n",
    "    transforms.ToTensor() # this also converts all pixel values from 0 to 255 to be between 0.0 and 1.0 \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c95274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_transformed_images(image_paths, transform, n = 3, seed = 42):\n",
    "    \"\"\"Plots a series of random images from image_paths.\n",
    "\n",
    "    Will open n image paths from image_paths, transform them\n",
    "    with transform and plot them side by side.\n",
    "\n",
    "    Args:\n",
    "        image_paths (list): List of target image paths. \n",
    "        transform (PyTorch Transforms): Transforms to apply to images.\n",
    "        n (int, optional): Number of images to plot. Defaults to 3.\n",
    "        seed (int, optional): Random seed for the random generator. Defaults to 42.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    random_image_path = random.sample(image_paths, k = n)\n",
    "    for image_path in random_image_path:\n",
    "        with Image.open(image_path) as f:\n",
    "            fig, ax = plt.subplots(1, 2)\n",
    "            ax[0].imshow(f) \n",
    "            ax[0].set_title(f\"Original \\nSize: {f.size}\")\n",
    "            ax[0].axis(\"off\")\n",
    "\n",
    "            # Transform and plot image\n",
    "            # Note: permute() will change shape of image to suit matplotlib \n",
    "            # (PyTorch default is [C, H, W] but Matplotlib is [H, W, C])\n",
    "            transformed_image = transform(f).permute(1, 2, 0)\n",
    "            ax[1].imshow(transformed_image) \n",
    "            ax[1].set_title(f\"Original \\nSize: {transformed_image.shape}\")\n",
    "            ax[1].axis(\"off\")\n",
    "\n",
    "            fig.suptitle(f\"Class: {image_path.parent.stem}\", fontsize=16)\n",
    "\n",
    "plot_transformed_images(image_path_list,\n",
    "                        transform = data_transform,\n",
    "                        n = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d05167",
   "metadata": {},
   "source": [
    "## 4. Option 1: Loading Image Data Using [`ImageFolder`](https://pytorch.org/vision/stable/generated/torchvision.datasets.ImageFolder.html#torchvision.datasets.ImageFolder)\n",
    "\n",
    "Alright, time to turn our image data into a `Dataset` capable of being used with PyTorch.\n",
    "\n",
    "Since our data is in standard image classification format, we can use the class [`torchvision.datasets.ImageFolder`](https://pytorch.org/vision/stable/generated/torchvision.datasets.ImageFolder.html#torchvision.datasets.ImageFolder).\n",
    "\n",
    "Where we can pass it the file path of a target image directory as well as a series of transforms we'd like to perform on our images.\n",
    "\n",
    "Let's test it out on our data folders `train_dir` and `test_dir` passing in `transform=data_transform` to turn our images into tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec1a793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use ImageFolder to create dataset(s)\n",
    "from torchvision import datasets\n",
    "train_data = datasets.ImageFolder(root = train_dir,\n",
    "                                  transform = data_transform,\n",
    "                                  target_transform = None) # transforms to perform on labels (if necessary)\n",
    "test_data = datasets.ImageFolder(root = test_dir,\n",
    "                                 transform = data_transform)\n",
    "\n",
    "print(f\"Train data:\\n{train_data}\\nTest data:\\n{test_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5a0143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get class names as a list\n",
    "class_names = train_data.classes\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ea6418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can also get class names as a dict\n",
    "class_dict = train_data.class_to_idx\n",
    "class_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6f4fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the lengths\n",
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328d89fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = train_data[0][0], train_data[0][1]\n",
    "print(f\"Image tensor:\\n{img}\")\n",
    "print(f\"Image shape: {img.shape}\")\n",
    "print(f\"Image datatype: {img.dtype}\")\n",
    "print(f\"Image label: {label}\")\n",
    "print(f\"Label datatype: {type(label)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cd6af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearrange the order of dimensions\n",
    "img_permute = img.permute(1, 2, 0)\n",
    "\n",
    "# Print out different shapes (before and after permute)\n",
    "print(f\"Original shape: {img.shape} -> [color_channels, height, width]\")\n",
    "print(f\"Image permute shape: {img_permute.shape} -> [height, width, color_channels]\")\n",
    "\n",
    "# Plot the image\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(img.permute(1, 2, 0))\n",
    "plt.axis(\"off\")\n",
    "plt.title(class_names[label], fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fe1a13",
   "metadata": {},
   "source": [
    "### 4.1 Turn loaded images into `DataLoader`'s\n",
    "\n",
    "We've got our images as PyTorch `Dataset`'s but now let's turn them into `DataLoader`'s.\n",
    "\n",
    "We'll do so using [`torch.utils.data.DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader).\n",
    "\n",
    "Turning our `Dataset`'s into `DataLoader`'s makes them iterable so a model can go through and learn the relationships between samples and targets (features and labels).\n",
    "\n",
    "To keep things simple, we'll use a `batch_size=1` and `num_workers=1`.\n",
    "\n",
    "What's `num_workers`?\n",
    "\n",
    "It defines how many subprocesses will be created to load your data.\n",
    "\n",
    "Think of it like this, the higher value `num_workers` is set to, the more compute power PyTorch will use to load your data.\n",
    "\n",
    "Personally, I usually set it to the total number of CPUs on my machine via Python's [`os.cpu_count()`](https://docs.python.org/3/library/os.html#os.cpu_count).\n",
    "\n",
    "This ensures the `DataLoader` recruits as many cores as possible to load data.\n",
    "\n",
    "> **Note:** There are more parameters you can get familiar with using `torch.utils.data.DataLoader` in the [PyTorch documentation](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b345102a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn train and test Datasets into DataLoaders\n",
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(dataset=train_data, \n",
    "                              batch_size=1, # how many samples per batch?\n",
    "                              num_workers=1, # how many subprocesses to use for data loading? (higher = more)\n",
    "                              shuffle=True) # shuffle the data?\n",
    "\n",
    "test_dataloader = DataLoader(dataset=test_data, \n",
    "                             batch_size=1, \n",
    "                             num_workers=1, \n",
    "                             shuffle=False) # don't usually need to shuffle testing data\n",
    "\n",
    "train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5e3da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = next(iter(train_dataloader))\n",
    "\n",
    "# Batch size will now be 1, try changing the batch_size parameter above and see what happens\n",
    "print(f\"Image shape: {img.shape} -> [batch_size, color_channels, height, width]\")\n",
    "print(f\"Label shape: {label.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311fbfa7",
   "metadata": {},
   "source": [
    "## 5. Option 2: Loading Image Data with a Custom `Dataset`\n",
    "\n",
    "What if a pre-built `Dataset` creator like [`torchvision.datasets.ImageFolder()`](https://pytorch.org/vision/stable/datasets.html#torchvision.datasets.ImageFolder) didn't exist?\n",
    "\n",
    "Or one for your specific problem didn't exist?\n",
    "\n",
    "Well, you could build your own.\n",
    "\n",
    "But wait, what are the pros and cons of creating your own custom way to load `Dataset`'s?\n",
    "\n",
    "| Pros of creating a custom `Dataset` | Cons of creating a custom `Dataset` |\n",
    "| ----- | ----- |\n",
    "| Can create a `Dataset` out of almost anything. | Even though you *could* create a `Dataset` out of almost anything, it doesn't mean it will work. | \n",
    "| Not limited to PyTorch pre-built `Dataset` functions. | Using a custom `Dataset` often results in writing more code, which could be prone to errors or performance issues. |\n",
    "\n",
    "To see this in action, let's work towards replicating `torchvision.datasets.ImageFolder()` by subclassing `torch.utils.data.Dataset` (the base class for all `Dataset`'s in PyTorch). \n",
    "\n",
    "We'll start by importing the modules we need:\n",
    "* Python's `os` for dealing with directories (our data is stored in directories).\n",
    "* Python's `pathlib` for dealing with filepaths (each of our images has a unique filepath).\n",
    "* `torch` for all things PyTorch.\n",
    "* PIL's `Image` class for loading images.\n",
    "* `torch.utils.data.Dataset` to subclass and create our own custom `Dataset`.\n",
    "* `torchvision.transforms` to turn our images into tensors.\n",
    "* Various types from Python's `typing` module to add type hints to our code.\n",
    "\n",
    "> **Note:** You can customize the following steps for your own dataset. The premise remains: write code to load your data in the format you'd like it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df35d6b5",
   "metadata": {},
   "source": [
    "`os.scandir` : Only scan all directories and files in the path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a464038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup path for target directory\n",
    "target_directory = train_dir\n",
    "print(f\"Target directory: {target_directory}\")\n",
    "\n",
    "# Get the class names from the target directory\n",
    "class_names_found = sorted([entry.name for entry in list(os.scandir(image_path / \"train\"))])\n",
    "print(f\"Class names found: {class_names_found}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fead7783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make function to find classes in target directory\n",
    "def find_classes(directory):\n",
    "    \"\"\"Finds the class folder names in a target directory.\n",
    "    \n",
    "    Assumes target directory is in standard image classification format.\n",
    "\n",
    "    Args:\n",
    "        directory (str): target directory to load classnames from.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[str], Dict[str, int]]: (list_of_class_names, dict(class_name: idx...))\n",
    "    \n",
    "    Example:\n",
    "        find_classes(\"food_images/train\")\n",
    "        >>> ([\"class_1\", \"class_2\"], {\"class_1\": 0, ...})\n",
    "    \"\"\"\n",
    "    # 1. Get the class names by scanning the target directory\n",
    "    classes = sorted(entry.name for entry in os.scandir(directory) if entry.is_dir())\n",
    "\n",
    "    # 2. Raise an error if class names not found\n",
    "    if not classes:\n",
    "        raise FileNotFoundError(f\"Couldn't find any classes in {directory}.\")\n",
    "    \n",
    "    # 3. Create a dictionary of index labels (computers prefer numerical rather than string labels)\n",
    "    class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n",
    "    return classes, class_to_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a959dfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_classes(train_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab183823",
   "metadata": {},
   "source": [
    "### 5.2 Create a custom `Dataset` to replicate `ImageFolder`\n",
    "\n",
    "Now we're ready to build our own custom `Dataset`.\n",
    "\n",
    "We'll build one to replicate the functionality of `torchvision.datasets.ImageFolder()`. \n",
    "\n",
    "This will be good practice, plus, it'll reveal a few of the required steps to make your own custom `Dataset`.\n",
    "\n",
    "It'll be a fair bit of a code... but nothing we can't handle!\n",
    "\n",
    "Let's break it down:\n",
    "1. Subclass `torch.utils.data.Dataset`.\n",
    "2. Initialize our subclass with a `targ_dir` parameter (the target data directory) and `transform` parameter (so we have the option to transform our data if needed).\n",
    "3. Create several attributes for `paths` (the paths of our target images), `transform` (the transforms we might like to use, this can be `None`), `classes` and `class_to_idx` (from our `find_classes()` function).\n",
    "4. Create a function to load images from file and return them, this could be using `PIL` or [`torchvision.io`](https://pytorch.org/vision/stable/io.html#image) (for input/output of vision data). \n",
    "5. Overwrite the `__len__` method of `torch.utils.data.Dataset` to return the number of samples in the `Dataset`, this is recommended but not required. This is so you can call `len(Dataset)`.\n",
    "6. Overwrite the `__getitem__` method of `torch.utils.data.Dataset` to return a single sample from the `Dataset`, this is required.\n",
    "\n",
    "Let's do it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17b4f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a custom dataset class (inherits from torch.utils.data.Dataset)\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# 1. Subclass torch.utils.data.Dataset\n",
    "class ImageFolderCustom(Dataset):\n",
    "\n",
    "    # 2. Initialize with a targ_dir and transform (optional) parameter\n",
    "    def __init__(self, targ_dir, transform = None):\n",
    "\n",
    "        # 3. Create class attributes\n",
    "        # Get all image paths\n",
    "        self.paths = list(Path(targ_dir).glob(\"*/*.jpg\")) # note: you'd have to update this if you've got .png's or .jpeg's\n",
    "        # Setup transforms\n",
    "        self.transform = transform\n",
    "        # Create classes and class_to_idx attributes\n",
    "        self.classes, self.class_to_idx = find_classes(targ_dir)\n",
    "\n",
    "    # 4. Make function to load images\n",
    "    def load_image(self, index):\n",
    "        image_path = self.paths[index]\n",
    "        return Image.open(image_path)\n",
    "    \n",
    "    # 5. Overwrite the __len__() method (optional but recommended for subclasses of torch.utils.data.Dataset)\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "    # 6. Overwrite the __getitem__() method (required for subclasses of torch.utils.data.Dataset)\n",
    "    def __getitem__(self, index):\n",
    "        img = self.load_image(index)\n",
    "        class_name = self.paths[index].parent.name # expects path in data_folder/class_name/image.jpeg\n",
    "        class_idx = self.class_to_idx[class_name]\n",
    "\n",
    "        # Transform if necessary\n",
    "        if self.transform:\n",
    "            return self.transform(img), class_idx  # return data, label (X, y)\n",
    "        else:\n",
    "            return img, class_idx # return data, label (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1dbc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment train data\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Don't augment test data, only reshape\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b11cb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_custom = ImageFolderCustom(targ_dir=train_dir, \n",
    "                                      transform=train_transforms)\n",
    "test_data_custom = ImageFolderCustom(targ_dir=test_dir, \n",
    "                                     transform=test_transforms)\n",
    "train_data_custom, test_data_custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6f60d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data_custom), len(test_data_custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f1fa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_custom.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d466493",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_custom.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9d4895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for equality amongst our custom Dataset and ImageFolder Dataset\n",
    "print((len(train_data_custom) == len(train_data)) & (len(test_data_custom) == len(test_data)))\n",
    "print(train_data_custom.classes == train_data.classes)\n",
    "print(train_data_custom.class_to_idx == train_data.class_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a14ab0",
   "metadata": {},
   "source": [
    "### 5.3 Create a function to display random images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ca0d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Take in a Dataset as well as a list of class names\n",
    "def display_random_images(dataset,\n",
    "                          classes = None,\n",
    "                          n = 10,\n",
    "                          display_shape = True,\n",
    "                          seed = None):\n",
    "    \n",
    "    if n > 5:\n",
    "        display_shape = False\n",
    "        print(f\"For display purposes, n shouldn't be larger than 10, setting to 10 and removing shape display.\")\n",
    "        \n",
    "    # 2. Adjust display if n too high\n",
    "    if n > 10:\n",
    "        n = 10\n",
    "        display_shape = False\n",
    "        print(f\"For display purposes, n shouldn't be larger than 10, setting to 10 and removing shape display.\")\n",
    "    \n",
    "    # 3. Set random seed\n",
    "    if seed:\n",
    "        random.seed(seed)\n",
    "\n",
    "    # 4. Get random sample indexes\n",
    "    random_samples_idx = random.sample(range(len(dataset)), k=n)\n",
    "\n",
    "    # 5. Setup plot\n",
    "    plt.figure(figsize=(16, 8))\n",
    "\n",
    "    # 6. Loop through samples and display random samples \n",
    "    for i, targ_sample in enumerate(random_samples_idx):\n",
    "        targ_image, targ_label = dataset[targ_sample][0], dataset[targ_sample][1]\n",
    "\n",
    "        # 7. Adjust image tensor shape for plotting: [color_channels, height, width] -> [color_channels, height, width]\n",
    "        targ_image_adjust = targ_image.permute(1, 2, 0)\n",
    "\n",
    "        # Plot adjusted samples\n",
    "        plt.subplot(1, n, i+1)\n",
    "        plt.imshow(targ_image_adjust)\n",
    "        plt.axis(\"off\")\n",
    "        if classes:\n",
    "            title = f\"class: {classes[targ_label]}\"\n",
    "            if display_shape:\n",
    "                title = title + f\"\\nshape: {targ_image_adjust.shape}\"\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975960fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display random images from ImageFolder created Dataset\n",
    "display_random_images(train_data, \n",
    "                      n=6, \n",
    "                      classes=class_names,\n",
    "                      seed=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ca29e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display random images from ImageFolderCustom Dataset\n",
    "display_random_images(train_data_custom, \n",
    "                      n=12, \n",
    "                      classes=class_names,\n",
    "                      seed=None) # Try setting the seed for reproducible images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24520751",
   "metadata": {},
   "source": [
    "### 5.4 Turn custom loaded images into `DataLoader`'s \n",
    "\n",
    "We've got a way to turn our raw images into `Dataset`'s (features mapped to labels or `X`'s mapped to `y`'s) through our `ImageFolderCustom` class.\n",
    "\n",
    "Now we could turn our custom `Dataset`'s into `DataLoader`'s\n",
    "\n",
    "Because our custom `Dataset`'s subclass `torch.utils.data.Dataset`, we can use them directly with `torch.utils.data.DataLoader()`.\n",
    "\n",
    "And we can do using very similar steps to before except this time we'll be using our custom created `Dataset`'s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4d6f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn train and test custom Dataset's into DataLoader's\n",
    "from torch.utils.data import DataLoader\n",
    "train_dataloader_custom = DataLoader(dataset=train_data_custom, # use custom created train Dataset\n",
    "                                     batch_size=1, # how many samples per batch?\n",
    "                                     num_workers=0, # how many subprocesses to use for data loading? (higher = more)\n",
    "                                     shuffle=True) # shuffle the data?\n",
    "\n",
    "test_dataloader_custom = DataLoader(dataset=test_data_custom, # use custom created test Dataset\n",
    "                                    batch_size=1, \n",
    "                                    num_workers=0, \n",
    "                                    shuffle=False) # don't usually need to shuffle testing data\n",
    "\n",
    "train_dataloader_custom, test_dataloader_custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b80025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get image and label from custom DataLoader\n",
    "img_custom, label_custom = next(iter(train_dataloader_custom))\n",
    "\n",
    "# Batch size will now be 1, try changing the batch_size parameter above and see what happens\n",
    "print(f\"Image shape: {img_custom.shape} -> [batch_size, color_channels, height, width]\")\n",
    "print(f\"Label shape: {label_custom.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd10796",
   "metadata": {},
   "source": [
    "## 6. Other forms of transforms (data augmentation)\n",
    "\n",
    "You can see them all in the [`torchvision.transforms` documentation](https://pytorch.org/vision/stable/transforms.html).\n",
    "\n",
    "The purpose of tranforms is to alter your images in some way.\n",
    "\n",
    "That may be turning your images into a tensor (as we've seen before).\n",
    "\n",
    "Or cropping it or randomly erasing a portion or randomly rotating them.\n",
    "\n",
    "Doing these kinds of transforms is often referred to as **data augmentation**.\n",
    "\n",
    "**Data augmentation** is the process of altering your data in such a way that you *artificially* increase the diversity of your training set.\n",
    "\n",
    "Training a model on this *artificially* altered dataset hopefully results in a model that is capable of better *generalization* (the patterns it learns are more robust to future unseen examples).\n",
    "\n",
    "You can see many different examples of data augmentation performed on images using `torchvision.transforms` in PyTorch's [Illustration of Transforms example](https://pytorch.org/vision/main/auto_examples/transforms/plot_transforms_illustrations.html).\n",
    "\n",
    "But let's try one out ourselves.\n",
    "\n",
    "Machine learning is all about harnessing the power of randomness and research shows that random transforms (like [`transforms.RandAugment()`](https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#randaugment) and [`transforms.TrivialAugmentWide()`](https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#trivialaugmentwide)) generally perform better than hand-picked transforms.\n",
    "\n",
    "The idea behind [TrivialAugment](https://arxiv.org/abs/2103.10158) is... well, trivial. \n",
    "\n",
    "You have a set of transforms and you randomly pick a number of them to perform on an image and at a random magnitude between a given range (a higher magnitude means more instense).\n",
    "\n",
    "The PyTorch team even [used TrivialAugment it to train their latest state-of-the-art vision models](https://pytorch.org/blog/how-to-train-state-of-the-art-models-using-torchvision-latest-primitives/#break-down-of-key-accuracy-improvements).\n",
    "\n",
    "![trivial augment data augmentation being used for PyTorch state of the art training](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-trivial-augment-being-using-in-PyTorch-resize.png)\n",
    "\n",
    "*TrivialAugment was one of the ingredients used in a recent state of the art training upgrade to various PyTorch vision models.*\n",
    "\n",
    "How about we test it out on some of our own images?\n",
    "\n",
    "The main parameter to pay attention to in `transforms.TrivialAugmentWide()` is `num_magnitude_bins=31`.\n",
    "\n",
    "It defines how much of a range an intensity value will be picked to apply a certain transform, `0` being no range and `31` being maximum range (highest chance for highest intensity). \n",
    "\n",
    "We can incorporate `transforms.TrivialAugmentWide()` into `transforms.Compose()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cc1344",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.TrivialAugmentWide(num_magnitude_bins= 31), # how intense \n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Don't need to perform augmentation on the test data\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), \n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edb61b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path_list = list(image_path.glob(\"*/*/*.jpg\"))\n",
    "\n",
    "plot_transformed_images(\n",
    "    image_paths= image_path_list,\n",
    "    transform = train_transforms,\n",
    "    n = 3,\n",
    "    seed = None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deac675c",
   "metadata": {},
   "source": [
    "## 7. Model 0: TinyVGG without data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66232368",
   "metadata": {},
   "source": [
    "### 7.1 Creating transforms and loading data for Model 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ed58d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simple transform\n",
    "simple_transform = transforms.Compose([ \n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfdf4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load and transform data\n",
    "from torchvision import datasets\n",
    "train_data_simple = datasets.ImageFolder(train_dir, transform = simple_transform)\n",
    "test_data_simple = datasets.ImageFolder(test_dir, transform = simple_transform)\n",
    "\n",
    "# 2. Turn data into DataLoaders\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Setup batch size and number of workers\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "print(f\"Creating DataLoader's with batch size {BATCH_SIZE} and {NUM_WORKERS} workers.\")\n",
    "\n",
    "# Create DataLoader's\n",
    "train_dataloader_simple = DataLoader(train_data_simple, batch_size = BATCH_SIZE, shuffle = True ,num_workers = 4)\n",
    "test_dataloader_simple = DataLoader(test_data_simple, batch_size = BATCH_SIZE, shuffle = False ,num_workers = 4)\n",
    "\n",
    "train_dataloader_simple, test_dataloader_simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b10248",
   "metadata": {},
   "source": [
    "### 7.2 Create TinyVGG model class\n",
    "\n",
    "In [notebook 03](https://www.learnpytorch.io/03_pytorch_computer_vision/#7-model-2-building-a-convolutional-neural-network-cnn), we used the TinyVGG model from the [CNN Explainer website](https://poloclub.github.io/cnn-explainer/).\n",
    "\n",
    "Let's recreate the same model, except this time we'll be using color images instead of grayscale (`in_channels=3` instead of `in_channels=1` for RGB pixels). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094bd8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyVGG(nn.Module):\n",
    "    \"\"\"\n",
    "    Model architecture copying TinyVGG from: \n",
    "    https://poloclub.github.io/cnn-explainer/\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape, hidden_units, output_shape):\n",
    "        super().__init__()\n",
    "        self.conv_block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = input_shape,\n",
    "                      out_channels = hidden_units,\n",
    "                      kernel_size = 3,\n",
    "                      stride = 1,\n",
    "                      padding = 1), # options = \"valid\" (no padding) or \"same\" (output has same shape as input) or int for specific number \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels = hidden_units,\n",
    "                      out_channels = hidden_units,\n",
    "                      kernel_size = 3,\n",
    "                      stride = 1,\n",
    "                      padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2,\n",
    "                         stride = 2)\n",
    "        )\n",
    "        self.conv_block_2 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_units, hidden_units, kernel_size = 3,padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_units, hidden_units, kernel_size = 3,padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features = hidden_units * 16 * 16,\n",
    "                      out_features = output_shape)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block_1(x)\n",
    "        x = self.conv_block_2(x)\n",
    "        return self.classifier(x)\n",
    "        # return self.classifier(self.conv_block_2(self.conv_block_1(x))) # <- leverage the benefits of operator fusion\n",
    "torch.manual_seed(42)\n",
    "model_0 = TinyVGG(3, 10, output_shape = len(train_data.classes)).to(device)\n",
    "\n",
    "model_0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd38451",
   "metadata": {},
   "source": [
    "> **Note:** One of the ways to speed up deep learning models computing on a GPU is to leverage **operator fusion**.\n",
    ">\n",
    "> This means in the `forward()` method in our model above, instead of calling a layer block and reassigning `x` every time, we call each block in succession (see the final line of the `forward()` method in the model above for an example).\n",
    ">\n",
    "> This saves the time spent reassigning `x` (memory heavy) and focuses on only computing on `x`.\n",
    "> \n",
    "> See [*Making Deep Learning Go Brrrr From First Principles*](https://horace.io/brrr_intro.html) by Horace He for more ways on how to speed up machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794ca4d7",
   "metadata": {},
   "source": [
    "### 7.3 Try a forward pass on a single image (to test the model)\n",
    "\n",
    "A good way to test a model is to do a forward pass on a single piece of data.\n",
    "\n",
    "It's also handy way to test the input and output shapes of our different layers.\n",
    "\n",
    "To do a forward pass on a single image, let's:\n",
    "1. Get a batch of images and labels from the `DataLoader`.\n",
    "2. Get a single image from the batch and `unsqueeze()` the image so it has a batch size of `1` (so its shape fits the model).\n",
    "3. Perform inference on a single image (making sure to send the image to the target `device`).\n",
    "4. Print out what's happening and convert the model's raw output logits to prediction probabilities with `torch.softmax()` (since we're working with multi-class data) and convert the prediction probabilities to prediction labels with `torch.argmax()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbdf5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Get a batch of images and labels from the DataLoader\n",
    "img_batch, label_batch = next(iter(train_dataloader_simple))\n",
    "\n",
    "# 2. Get a single image from the batch and unsqueeze the image so its shape fits the model\n",
    "img_single, label_single = img_batch[0].unsqueeze(dim = 0), label_batch[0]\n",
    "print(f\"Single image shape: {img_single.shape}\\n\")\n",
    "\n",
    "# 3. Perform a forward pass on a single image\n",
    "model_0.eval()\n",
    "with torch.inference_mode():\n",
    "    pred = model_0(img_single.to(device))\n",
    "\n",
    "# 4. Print out what's happening and convert model logits -> pred probs -> pred label\n",
    "print(f\"Output logits:\\n{pred}\\n\")\n",
    "print(f\"Output prediction probabilities:\\n{torch.softmax(pred, dim = 1)}\\n\")\n",
    "print(f\"Output prediction label:\\n{torch.argmax(pred, dim = 1)}\\n\")\n",
    "print(f\"Actual label:\\n{label_single}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246a25d8",
   "metadata": {},
   "source": [
    "### 7.4 Use `torchinfo` to get an idea of the shapes going through our model\n",
    "\n",
    "Printing out our model with `print(model)` gives us an idea of what's going on with our model.\n",
    "\n",
    "And we can print out the shapes of our data throughout the `forward()` method.\n",
    "\n",
    "However, a helpful way to get information from our model is to use [`torchinfo`](https://github.com/TylerYep/torchinfo).\n",
    "\n",
    "`torchinfo` comes with a `summary()` method that takes a PyTorch model as well as an `input_shape` and returns what happens as a tensor moves through your model.\n",
    "\n",
    "> **Note:** If you're using Google Colab, you'll need to install `torchinfo`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194050a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install torchinfo if it's not available, import it if it is\n",
    "try: \n",
    "    import torchinfo\n",
    "except:\n",
    "    !pip install torchinfo\n",
    "    import torchinfo\n",
    "    \n",
    "from torchinfo import summary\n",
    "summary(model_0, input_size=[1, 3, 64, 64]) # do a test pass through of an example input size "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38804741",
   "metadata": {},
   "source": [
    "### 7.5 Create train & test loop functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bba989",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model: torch.nn.Module, \n",
    "               dataloader: torch.utils.data.DataLoader, \n",
    "               loss_fn: torch.nn.Module, \n",
    "               optimizer: torch.optim.Optimizer):\n",
    "    # Put model in train mode\n",
    "    model.train()\n",
    "    \n",
    "    # Setup train loss and train accuracy values\n",
    "    train_loss, train_acc = 0, 0\n",
    "    \n",
    "    # Loop through data loader data batches\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Send data to target device\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # 1. Forward pass\n",
    "        y_pred = model(X)\n",
    "\n",
    "        # 2. Calculate  and accumulate loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item() \n",
    "\n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate and accumulate accuracy metrics across all batches\n",
    "        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "        train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
    "\n",
    "    # Adjust metrics to get average loss and accuracy per batch \n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_acc = train_acc / len(dataloader)\n",
    "    return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fc1fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(model: torch.nn.Module, \n",
    "              dataloader: torch.utils.data.DataLoader, \n",
    "              loss_fn: torch.nn.Module):\n",
    "    # Put model in eval mode\n",
    "    model.eval() \n",
    "    \n",
    "    # Setup test loss and test accuracy values\n",
    "    test_loss, test_acc = 0, 0\n",
    "    \n",
    "    # Turn on inference context manager\n",
    "    with torch.inference_mode():\n",
    "        # Loop through DataLoader batches\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            # Send data to target device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "    \n",
    "            # 1. Forward pass\n",
    "            test_pred_logits = model(X)\n",
    "\n",
    "            # 2. Calculate and accumulate loss\n",
    "            loss = loss_fn(test_pred_logits, y)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            # Calculate and accumulate accuracy\n",
    "            test_pred_labels = test_pred_logits.argmax(dim=1)\n",
    "            test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
    "            \n",
    "    # Adjust metrics to get average loss and accuracy per batch \n",
    "    test_loss = test_loss / len(dataloader)\n",
    "    test_acc = test_acc / len(dataloader)\n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2b47f2",
   "metadata": {},
   "source": [
    "### 7.6 Creating a `train()` function to combine `train_step()` and `test_step()`\n",
    "\n",
    "1. Take in a model, a `DataLoader` for training and test sets, an optimizer, a loss function and how many epochs to perform each train and test step for.\n",
    "2. Create an empty results dictionary for `train_loss`, `train_acc`, `test_loss` and `test_acc` values (we can fill this up as training goes on).\n",
    "3. Loop through the training and test step functions for a number of epochs.\n",
    "4. Print out what's happening at the end of each epoch.\n",
    "5. Update the empty results dictionary with the updated metrics each epoch.\n",
    "6. Return the filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b12e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train(model, train_dataloader, test_dataloader, optimizer, loss_fn, epochs = 5):\n",
    "    \n",
    "    results = {\"train_loss\": [],\n",
    "               \"train_acc\": [],\n",
    "               \"test_loss\": [],\n",
    "               \"test_acc\": []\n",
    "    }\n",
    "    \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_step(model = model,\n",
    "                                           dataloader = train_dataloader,\n",
    "                                           loss_fn = loss_fn,\n",
    "                                           optimizer = optimizer)\n",
    "        test_loss, test_acc = test_step(model = model, dataloader = test_dataloader, loss_fn = loss_fn)\n",
    "        \n",
    "        print(\n",
    "            f\"Epoch: {epoch+1} | \"\n",
    "            f\"train_loss: {train_loss:.4f} | \"\n",
    "            f\"train_acc: {train_acc:.4f} | \"\n",
    "            f\"test_loss: {test_loss:.4f} | \"\n",
    "            f\"test_acc: {test_acc:.4f}\"\n",
    "        )\n",
    "        \n",
    "        results[\"train_loss\"].append(train_loss.item() if isinstance(train_loss, torch.Tensor) else train_loss)\n",
    "        results[\"train_acc\"].append(train_acc.item() if isinstance(train_acc, torch.Tensor) else train_acc)\n",
    "        results[\"test_loss\"].append(test_loss.item() if isinstance(test_loss, torch.Tensor) else test_loss)\n",
    "        results[\"test_acc\"].append(test_acc.item() if isinstance(test_acc, torch.Tensor) else test_acc)\n",
    "\n",
    "    # 6. Return the filled results at the end of the epochs\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c73119a",
   "metadata": {},
   "source": [
    "### 7.7 Train and Evaluate Model 0\n",
    "\n",
    "To see how long things take, we'll import Python's [`timeit.default_timer()`](https://docs.python.org/3/library/timeit.html#timeit.default_timer) method to calculate the training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0053b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds\n",
    "torch.manual_seed(42) \n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "# Set number of epochs\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "# Recreate an instance of TinyVGG\n",
    "model_0 = TinyVGG(input_shape=3, # number of color channels (3 for RGB) \n",
    "                  hidden_units=10, \n",
    "                  output_shape=len(train_data.classes)).to(device)\n",
    "\n",
    "# Setup loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_0.parameters(), lr = 0.001)\n",
    "\n",
    "# Start the timer\n",
    "from timeit import default_timer as timer \n",
    "start_time = timer()\n",
    "\n",
    "# Train model_0 \n",
    "model_0_results = train(model=model_0, \n",
    "                        train_dataloader=train_dataloader_simple,\n",
    "                        test_dataloader=test_dataloader_simple,\n",
    "                        optimizer=optimizer,\n",
    "                        loss_fn=loss_fn, \n",
    "                        epochs=NUM_EPOCHS)\n",
    "\n",
    "# End the timer and print out how long it took\n",
    "end_time = timer()\n",
    "print(f\"Total training time: {end_time-start_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab662082",
   "metadata": {},
   "source": [
    "### 7.8 Plot the loss curves of Model 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598afecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0_results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625fc90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curves(results):\n",
    "    \"\"\"Plots training curves of a results dictionary.\n",
    "\n",
    "    Args:\n",
    "        results (dict): dictionary containing list of values, e.g.\n",
    "            {\"train_loss\": [...],\n",
    "             \"train_acc\": [...],\n",
    "             \"test_loss\": [...],\n",
    "             \"test_acc\": [...]}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the loss values of the results dictionary (training and test)\n",
    "    loss = results['train_loss']\n",
    "    test_loss = results['test_loss']\n",
    "\n",
    "    # Get the accuracy values of the results dictionary (training and test)\n",
    "    accuracy = results['train_acc']\n",
    "    test_accuracy = results['test_acc']\n",
    "\n",
    "    # Figure out how many epochs there were\n",
    "    epochs = range(len(results['train_loss']))\n",
    "\n",
    "    # Setup a plot \n",
    "    plt.figure(figsize=(15, 7))\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, loss, label='train_loss')\n",
    "    plt.plot(epochs, test_loss, label='test_loss')\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, accuracy, label='train_accuracy')\n",
    "    plt.plot(epochs, test_accuracy, label='test_accuracy')\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38390e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curves(model_0_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401f1645",
   "metadata": {},
   "source": [
    "## 8. What should an ideal loss curve look like?\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-loss-curves-overfitting-underfitting-ideal.jpg\" alt=\"different training and test loss curves illustrating overfitting, underfitting and the ideal loss curves\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80249457",
   "metadata": {},
   "source": [
    "### 8.1 How to deal with overfitting\n",
    "\n",
    "Since the main problem with overfitting is that your model is fitting the training data *too well*, you'll want to use techniques to \"reign it in\".\n",
    "\n",
    "A common technique of preventing overfitting is known as [**regularization**](https://ml-cheatsheet.readthedocs.io/en/latest/regularization.html).\n",
    "\n",
    "I like to think of this as \"making our models more regular\", as in, capable of fitting *more* kinds of data.\n",
    "\n",
    "Let's discuss a few methods to prevent overfitting.\n",
    "\n",
    "| **Method to prevent overfitting** | **What is it?** |\n",
    "| ----- | ----- |\n",
    "| **Get more data** | Having more data gives the model more opportunities to learn patterns, patterns which may be more generalizable to new examples. | \n",
    "| **Simplify your model** | If the current model is already overfitting the training data, it may be too complicated of a model. This means it's learning the patterns of the data too well and isn't able to generalize well to unseen data. One way to simplify a model is to reduce the number of layers it uses or to reduce the number of hidden units in each layer. | \n",
    "| **Use data augmentation** | [**Data augmentation**](https://developers.google.com/machine-learning/glossary#data-augmentation) manipulates the training data in a way so that's harder for the model to learn as it artificially adds more variety to the data. If a model is able to learn patterns in augmented data, the model may be able to generalize better to unseen data. |\n",
    "| **Use transfer learning** | [**Transfer learning**](https://developers.google.com/machine-learning/glossary#transfer-learning) involves leveraging the patterns (also called pretrained weights) one model has learned to use as the foundation for your own task. In our case, we could use one computer vision model pretrained on a large variety of images and then tweak it slightly to be more specialized for food images. |\n",
    "| **Use dropout layers** | Dropout layers randomly remove connections between hidden layers in neural networks, effectively simplifying a model but also making the remaining connections better. See [`torch.nn.Dropout()`](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html) for more. | \n",
    "| **Use learning rate decay** | The idea here is to slowly decrease the learning rate as a model trains. This is akin to reaching for a coin at the back of a couch. The closer you get, the smaller your steps. The same with the learning rate, the closer you get to [**convergence**](https://developers.google.com/machine-learning/glossary#convergence), the smaller you'll want your weight updates to be.  |\n",
    "| **Use early stopping** | [**Early stopping**](https://developers.google.com/machine-learning/glossary#early_stopping) stops model training *before* it begins to overfit. As in, say the model's loss has stopped decreasing for the past 10 epochs (this number is arbitrary), you may want to stop the model training here and go with the model weights that had the lowest loss (10 epochs prior). |\n",
    "\n",
    "There are more methods for dealing with overfitting but these are some of the main ones.\n",
    "\n",
    "As you start to build more and more deep models, you'll find because deep learnings are *so good* at learning patterns in data, dealing with overfitting is one of the primary problems of deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8ce4b4",
   "metadata": {},
   "source": [
    "### 8.2 How to deal with underfitting \n",
    "\n",
    "When a model is [**underfitting**](https://developers.google.com/machine-learning/glossary#underfitting) it is considered to have poor predictive power on the training and test sets.\n",
    "\n",
    "In essence, an underfitting model will fail to reduce the loss values to a desired level.\n",
    "\n",
    "Right now, looking at our current loss curves, I'd considered our `TinyVGG` model, `model_0`, to be underfitting the data.\n",
    "\n",
    "The main idea behind dealing with underfitting is to *increase* your model's predictive power.\n",
    "\n",
    "There are several ways to do this.\n",
    "\n",
    "| **Method to prevent underfitting** | **What is it?** |\n",
    "| ----- | ----- |\n",
    "| **Add more layers/units to your model** | If your model is underfitting, it may not have enough capability to *learn* the required patterns/weights/representations of the data to be predictive. One way to add more predictive power to your model is to increase the number of hidden layers/units within those layers. | \n",
    "| **Tweak the learning rate** | Perhaps your model's learning rate is too high to begin with. And it's trying to update its weights each epoch too much, in turn not learning anything. In this case, you might lower the learning rate and see what happens. |\n",
    "| **Use transfer learning** | Transfer learning is capable of preventing overfitting and underfitting. It involves using the patterns from a previously working model and adjusting them to your own problem. |\n",
    "| **Train for longer** | Sometimes a model just needs more time to learn representations of data. If you find in your smaller experiments your model isn't learning anything, perhaps leaving it train for a more epochs may result in better performance. |\n",
    "| **Use less regularization** | Perhaps your model is underfitting because you're trying to prevent overfitting too much. Holding back on regularization techniques can help your model fit the data better. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da83e6e4",
   "metadata": {},
   "source": [
    "## 9. Model 1: TinyVGG with Data Augmentation\n",
    "\n",
    "This time, let's load in the data and use **data augmentation** to see if it improves our results in anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2284295c",
   "metadata": {},
   "source": [
    "### 9.1 Create transform with data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee977a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training transform with TrivialAugment\n",
    "train_transform_trivial_augment = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.TrivialAugmentWide(num_magnitude_bins = 31),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Create testing transform (no data augmentation)\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60725a3f",
   "metadata": {},
   "source": [
    "### 9.2 Create train and test `Dataset`'s and `DataLoader`'s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e29649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn image folders into Datasets\n",
    "train_data_augmented = datasets.ImageFolder(train_dir, transform=train_transform_trivial_augment)\n",
    "test_data_simple = datasets.ImageFolder(test_dir, transform=test_transform)\n",
    "\n",
    "train_data_augmented, test_data_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29ebe51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn Datasets into DataLoader's\n",
    "import os\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "\n",
    "torch.manual_seed(42)\n",
    "train_dataloader_augmented = DataLoader(train_data_augmented, \n",
    "                                        batch_size=BATCH_SIZE, \n",
    "                                        shuffle=True,\n",
    "                                        num_workers= 4)\n",
    "\n",
    "test_dataloader_simple = DataLoader(test_data_simple, \n",
    "                                    batch_size=BATCH_SIZE, \n",
    "                                    shuffle=False, \n",
    "                                    num_workers=4)\n",
    "\n",
    "train_dataloader_augmented, test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ced42e",
   "metadata": {},
   "source": [
    "### 9.3 Construct and train Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79a316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model_1 and send it to the target device\n",
    "torch.manual_seed(42)\n",
    "model_1 = TinyVGG(\n",
    "    input_shape=3,\n",
    "    hidden_units=10,\n",
    "    output_shape=len(train_data_augmented.classes)).to(device)\n",
    "model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07851cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds\n",
    "torch.manual_seed(42) \n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "# Set number of epochs\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "# Setup loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model_1.parameters(), lr=0.001)\n",
    "\n",
    "# Start the timer\n",
    "from timeit import default_timer as timer \n",
    "start_time = timer()\n",
    "\n",
    "# Train model_1\n",
    "model_1_results = train(model=model_1, \n",
    "                        train_dataloader=train_dataloader_augmented,\n",
    "                        test_dataloader=test_dataloader_simple,\n",
    "                        optimizer=optimizer,\n",
    "                        loss_fn=loss_fn, \n",
    "                        epochs=NUM_EPOCHS)\n",
    "\n",
    "# End the timer and print out how long it took\n",
    "end_time = timer()\n",
    "print(f\"Total training time: {end_time-start_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e38eafd",
   "metadata": {},
   "source": [
    "### 9.4 Plot the loss curves of Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ee3572",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curves(model_1_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e0b0df",
   "metadata": {},
   "source": [
    "## 10. Compare model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2af145",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "model_0_df = pd.DataFrame(model_0_results)\n",
    "model_1_df = pd.DataFrame(model_1_results)\n",
    "model_0_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7e4eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup a plot \n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Get number of epochs\n",
    "epochs = range(len(model_0_df))\n",
    "\n",
    "# Plot train loss\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(epochs, model_0_df[\"train_loss\"], label=\"Model 0\")\n",
    "plt.plot(epochs, model_1_df[\"train_loss\"], label=\"Model 1\")\n",
    "plt.title(\"Train Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot test loss\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(epochs, model_0_df[\"test_loss\"], label=\"Model 0\")\n",
    "plt.plot(epochs, model_1_df[\"test_loss\"], label=\"Model 1\")\n",
    "plt.title(\"Test Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot train accuracy\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(epochs, model_0_df[\"train_acc\"], label=\"Model 0\")\n",
    "plt.plot(epochs, model_1_df[\"train_acc\"], label=\"Model 1\")\n",
    "plt.title(\"Train Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot test accuracy\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(epochs, model_0_df[\"test_acc\"], label=\"Model 0\")\n",
    "plt.plot(epochs, model_1_df[\"test_acc\"], label=\"Model 1\")\n",
    "plt.title(\"Test Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9387d6",
   "metadata": {},
   "source": [
    "## 11. Make a prediction on a custom image\n",
    "\n",
    "We load an image and then **preprocess it in a way that matches the type of data our model was trained on**.\n",
    "\n",
    "The image comes from [my Dad giving two thumbs up to a big pizza from the Learn PyTorch for Deep Learning GitHub](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/images/04-pizza-dad.jpeg).\n",
    "\n",
    "We download the image using Python's `requests` module.\n",
    "\n",
    "> **Note:** If you're using Google Colab, you can also upload an image to the current session by going to the left hand side menu -> Files -> Upload to session storage. Beware though, this image will delete when your Google Colab session ends.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be4a310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download custom image\n",
    "import requests\n",
    "\n",
    "custom_image_path = data_path / \"04-pizza-dad.jpeg\"\n",
    "\n",
    "if not custom_image_path.is_file():\n",
    "    with open(custom_image_path, 'wb') as f:\n",
    "        request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\")\n",
    "        print(f\"Downloading {custom_image_path}...\")\n",
    "        f.write(request.content)\n",
    "else:\n",
    "    print(f\"{custom_image_path} already exists, skipping download.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b811b795",
   "metadata": {},
   "source": [
    "### 11.1 Loading in a custom image with PyTorch\n",
    "\n",
    "Now we can load the image.\n",
    "\n",
    "PyTorch's `torchvision` has several input and output (\"IO\" or \"io\" for short) methods for reading and writing images and video in [`torchvision.io`](https://pytorch.org/vision/stable/io.html).\n",
    "\n",
    "Since we want to load in an image, we'll use [`torchvision.io.read_image()`](https://pytorch.org/vision/stable/generated/torchvision.io.read_image.html#torchvision.io.read_image).\n",
    "\n",
    "This method will read a JPEG or PNG image and turn it into a 3 dimensional RGB or grayscale `torch.Tensor` with values of datatype `uint8` in range `[0, 255]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dee802",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "# Read in custom image\n",
    "custom_image_uint8 = torchvision.io.read_image(str(custom_image_path))\n",
    "\n",
    "# Print cout image data\n",
    "print(f\"Custom image tensor:\\n{custom_image_uint8}\\n\")\n",
    "print(f\"Custom image shape: {custom_image_uint8.shape}\\n\")\n",
    "print(f\"Custom image dtype: {custom_image_uint8.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938bcfff",
   "metadata": {},
   "source": [
    "Nice! Looks like our image is in tensor format, however, is this image format compatible with our model?\n",
    "\n",
    "Our `custom_image` tensor is of datatype `torch.uint8` and its values are between `[0, 255]`.\n",
    "\n",
    "But our model takes image tensors of datatype `torch.float32` and with values between `[0, 1]`.\n",
    "\n",
    "So before we use our custom image with our model, **we'll need to convert it to the same format as the data our model is trained on**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee152fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to make a prediction on image in uint8 format (this will error)\n",
    "# We need input to be float32\n",
    "model_1.eval()\n",
    "with torch.inference_mode():\n",
    "    model_1(custom_image_uint8.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396c5c6e",
   "metadata": {},
   "source": [
    "If we try to make a prediction on an image in a different datatype to what our model was trained on, we get an error like the following:\n",
    "\n",
    "> `RuntimeError: Input type (torch.cuda.ByteTensor) and weight type (torch.cuda.FloatTensor) should be the same`\n",
    "\n",
    "Let's fix this by converting our custom image to the same datatype as what our model was trained on (`torch.float32`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd6bd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in custom image and convert the tensor values to float32\n",
    "custom_image = torchvision.io.read_image(str(custom_image_path)).type(torch.float32)\n",
    "\n",
    "# Divide the image pixel values by 255 to get them between [0, 1]\n",
    "custom_image = custom_image / 255.\n",
    "\n",
    "print(f\"Custom image tensor:\\n{custom_image}\\n\")\n",
    "print(f\"Custom image shape: {custom_image.shape}\\n\")\n",
    "print(f\"Custom image dtype: {custom_image.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b76567",
   "metadata": {},
   "source": [
    "### 11.2 Predicting on custom images with a trained PyTorch model\n",
    "\n",
    "One more problem, the `shape` is not same as the input shape in our setting.\n",
    "\n",
    "Our model was trained on images with shape `[3, 64, 64]`, whereas our custom image is currently `[3, 4032, 3024]`. \n",
    "\n",
    "How could we make sure our custom image is the same shape as the images our model was trained on?\n",
    "\n",
    "Are there any `torchvision.transforms` that could help?\n",
    "\n",
    "Before we answer that question, let's plot the image with `matplotlib` to make sure it looks okay, remember we'll have to permute the dimensions from `CHW` to `HWC` to suit `matplotlib`'s requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9856c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot custom image\n",
    "plt.imshow(custom_image.permute(1, 2, 0)) # need to permute image dimensions from CHW -> HWC otherwise matplotlib will error\n",
    "plt.title(f\"Image shape: {custom_image.shape}\")\n",
    "plt.axis(False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13b7735",
   "metadata": {},
   "source": [
    "One way to do so is with `torchvision.transforms.Resize()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4ad2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create transform pipleine to resize image\n",
    "custom_image_transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64))\n",
    "])\n",
    "\n",
    "# Transform target image\n",
    "custom_image_transformed = custom_image_transform(custom_image)\n",
    "\n",
    "# Print out original shape and new shape\n",
    "print(f\"Original shape: {custom_image.shape}\")\n",
    "print(f\"New shape: {custom_image_transformed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590aefb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.eval()\n",
    "with torch.inference_mode():\n",
    "    custom_image_pred = model_1(custom_image_transformed.unsqueeze(dim = 0).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e9e1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_image_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe2c0a9",
   "metadata": {},
   "source": [
    "> **Note:** What we've just gone through are three of the classical and most common deep learning and PyTorch issues:\n",
    "> 1. **Wrong datatypes** - our model expects `torch.float32` where our original custom image was `uint8`.\n",
    "> 2. **Wrong device** - our model was on the target `device` (in our case, the GPU) whereas our target data hadn't been moved to the target `device` yet.\n",
    "> 3. **Wrong shapes** - our model expected an input image of shape `[N, C, H, W]` or `[batch_size, color_channels, height, width]` whereas our custom image tensor was of shape `[color_channels, height, width]`.\n",
    ">\n",
    "> Keep in mind, these errors aren't just for predicting on custom images. \n",
    ">\n",
    "> They will be present with almost every kind of data type (text, audio, structured data) and problem you work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ed9528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out prediction logits\n",
    "print(f\"Prediction logits: {custom_image_pred}\")\n",
    "\n",
    "# Convert logits -> prediction probabilities (using torch.softmax() for multi-class classification)\n",
    "custom_image_pred_probs = torch.softmax(custom_image_pred, dim=1)\n",
    "print(f\"Prediction probabilities: {custom_image_pred_probs}\")\n",
    "\n",
    "# Convert prediction probabilities -> prediction labels\n",
    "custom_image_pred_label = torch.argmax(custom_image_pred_probs, dim=1)\n",
    "print(f\"Prediction label: {custom_image_pred_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2f7b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the predicted label\n",
    "custom_image_pred_class = class_names[custom_image_pred_label.item()]\n",
    "custom_image_pred_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667d187c",
   "metadata": {},
   "source": [
    "### 11.3 Putting custom image prediction together: building a function\n",
    "\n",
    "Doing all of the above steps every time you'd like to make a prediction on a custom image would quickly become tedious.\n",
    "\n",
    "So let's put them all together in a function we can easily use over and over again.\n",
    "\n",
    "Specifically, let's make a function that:\n",
    "1. Takes in a target image path and converts to the right datatype for our model (`torch.float32`).\n",
    "2. Makes sure the target image pixel values are in the range `[0, 1]`.\n",
    "3. Transforms the target image if necessary.\n",
    "4. Makes sure the model is on the target device.\n",
    "5. Makes a prediction on the target image with a trained model (ensuring the image is the right size and on the same device as the model).\n",
    "6. Converts the model's output logits to prediction probabilities.\n",
    "7. Converts the prediction probabilities to prediction labels.\n",
    "8. Plots the target image alongside the model prediction and prediction probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34ff126",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_and_plot_image(model,\n",
    "                        image_path,\n",
    "                        class_names,\n",
    "                        transform = None,\n",
    "                        device = device):\n",
    "    \"\"\"Makes a prediction on a target image and plots the image with its prediction.\"\"\"\n",
    "\n",
    "    # 1. Load in image and convert the tensor values to float32\n",
    "    target_image = torchvision.io.read_image(str(image_path)).type(torch.float32)\n",
    "\n",
    "    # 2. Divide the image pixel values by 255 to get them between [0, 1]\n",
    "    target_image = target_image / 255.\n",
    "\n",
    "    # 3. Transform if necessary\n",
    "    if transform:\n",
    "        target_image = transform(target_image)\n",
    "\n",
    "    # 4. Make sure the model is on the target device\n",
    "    model.to(device)\n",
    "\n",
    "    # 5. Turn on model evaluation mode and inference mode\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        # Add an extra dimension to the image\n",
    "        target_image = target_image.unsqueeze(dim = 0)\n",
    "\n",
    "        # Make a prediction on image with an extra dimension and send it to the target device\n",
    "        target_image_pred = model(target_image.to(device))\n",
    "\n",
    "    # 6. Convert logits -> prediction probabilities (using torch.softmax() for multi-class classification)\n",
    "    target_image_pred_probs = torch.softmax(target_image_pred, dim = 1)\n",
    "\n",
    "    # 7. Convert prediction probabilities -> prediction labels\n",
    "    target_image_pred_label = target_image_pred_probs.argmax(dim = 1)\n",
    "\n",
    "    # 8. Plot the image alongside the prediction and prediction probability\n",
    "    plt.imshow(target_image.squeeze().permute(1, 2, 0)) # make sure it's the right size for matplotlib\n",
    "    if class_names:\n",
    "        title = f\"Pred: {class_names[target_image_pred_label.item()]} | Prob: {target_image_pred_probs.max().item():.3f}\"\n",
    "    else:\n",
    "        title = f\"Pred: {target_image_pred_label} | Prob: {target_image_pred_probs.max().item():.3f}\"\n",
    "    plt.title(title)\n",
    "    plt.axis(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86813dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pred on our custom image\n",
    "pred_and_plot_image(model=model_1,\n",
    "                    image_path=custom_image_path,\n",
    "                    class_names=class_names,\n",
    "                    transform=custom_image_transform,\n",
    "                    device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de4e710",
   "metadata": {},
   "source": [
    "## Main takeaways\n",
    "\n",
    "Let's summarise it with a few dot points.\n",
    "\n",
    "* PyTorch has many in-built functions to deal with all kinds of data, from vision to text to audio to recommendation systems.\n",
    "* If PyTorch's built-in data loading functions don't suit your requirements, you can write code to create your own custom datasets by subclassing `torch.utils.data.Dataset`.\n",
    "* `torch.utils.data.DataLoader`'s in PyTorch help turn your `Dataset`'s into iterables that can be used when training and testing a model.\n",
    "* A lot of machine learning is dealing with the balance between **overfitting** and **underfitting** (we discussed different methods for each above, so a good exercise would be to research more and writing code to try out the different techniques).\n",
    "* Predicting on your own custom data with a trained model is possible, as long as you format the data into a similar format to what the model was trained on. Make sure you take care of the three big PyTorch and deep learning errors:\n",
    "    1. **Wrong datatypes** - Your model expected `torch.float32` when your data is `torch.uint8`.\n",
    "    2. **Wrong data shapes** - Your model expected `[batch_size, color_channels, height, width]` when your data is `[color_channels, height, width]`.\n",
    "    3. **Wrong devices** - Your model is on the GPU but your data is on the CPU. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b70931",
   "metadata": {},
   "source": [
    "# END!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f479687",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
